Model = MetaModel(
  (criterion): CrossEntropyLoss()
  (llma): Transformer(
    (tok_embeddings): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=False)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=False)
          (w2): Linear(in_features=11008, out_features=4096, bias=False)
          (w3): Linear(in_features=4096, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
    )
    (norm): RMSNorm()
    (output): Linear(in_features=4096, out_features=32000, bias=False)
    (clip): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (patch_dropout): Identity()
        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): ModuleList(
            (0-23): 24 x ResidualAttentionBlock(
              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (ls_1): Identity()
              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (ls_2): Identity()
            )
          )
        )
        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): None
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (resample_layers): ModuleDict(
      (0): ModuleList(
        (0-7): 8 x TransformerBlock(
          (attention): Attention(
            (wq): Linear(in_features=1024, out_features=1024, bias=False)
            (wk): Linear(in_features=1024, out_features=1024, bias=False)
            (wv): Linear(in_features=1024, out_features=1024, bias=False)
            (wo): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (feed_forward): FeedForward(
            (w1): Linear(in_features=1024, out_features=2816, bias=False)
            (w2): Linear(in_features=2816, out_features=1024, bias=False)
            (w3): Linear(in_features=1024, out_features=2816, bias=False)
          )
          (attention_norm): RMSNorm()
          (ffn_norm): RMSNorm()
        )
      )
      (1): ModuleList(
        (0-7): 8 x TransformerBlock(
          (attention): Attention(
            (wq): Linear(in_features=1024, out_features=1024, bias=False)
            (wk): Linear(in_features=1024, out_features=1024, bias=False)
            (wv): Linear(in_features=1024, out_features=1024, bias=False)
            (wo): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (feed_forward): FeedForward(
            (w1): Linear(in_features=1024, out_features=2816, bias=False)
            (w2): Linear(in_features=2816, out_features=1024, bias=False)
            (w3): Linear(in_features=1024, out_features=2816, bias=False)
          )
          (attention_norm): RMSNorm()
          (ffn_norm): RMSNorm()
        )
      )
      (2): ModuleList(
        (0-7): 8 x TransformerBlock(
          (attention): Attention(
            (wq): Linear(in_features=1024, out_features=1024, bias=False)
            (wk): Linear(in_features=1024, out_features=1024, bias=False)
            (wv): Linear(in_features=1024, out_features=1024, bias=False)
            (wo): Linear(in_features=1024, out_features=1024, bias=False)
          )
          (feed_forward): FeedForward(
            (w1): Linear(in_features=1024, out_features=2816, bias=False)
            (w2): Linear(in_features=2816, out_features=1024, bias=False)
            (w3): Linear(in_features=1024, out_features=2816, bias=False)
          )
          (attention_norm): RMSNorm()
          (ffn_norm): RMSNorm()
        )
      )
    )
    (conv1): ModuleDict(
      (audio): Conv2d(1, 1024, kernel_size=(16, 16), stride=(10, 10))
      (point): PointPatchEmbed(
        (grouper): KNNGroup(
          (knn): KNN()
        )
        (conv1): Conv2d(6, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (fmri): Linear(in_features=15724, out_features=8192, bias=True)
      (imu): Conv1d(6, 1024, kernel_size=(10,), stride=(1,), bias=False)
    )
    (positional_embedding): ParameterDict(
        (audio): Parameter containing: [torch.cuda.HalfTensor of size 1213x1024 (GPU 0)]
        (point): Parameter containing: [torch.cuda.HalfTensor of size 1025x1024 (GPU 0)]
        (fmri): Parameter containing: [torch.cuda.HalfTensor of size 9x1024 (GPU 0)]
        (imu): Parameter containing: [torch.cuda.HalfTensor of size 392x1024 (GPU 0)]
    )
    (resample_tokens): ParameterDict(
        (image): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (video): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (audio): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (point): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (rgbd): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (rgbn): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (fmri): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
        (imu): Parameter containing: [torch.cuda.HalfTensor of size 1x30x1024 (GPU 0)]
    )
    (clip_proj1): ModuleDict(
      (image): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (video): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (audio): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (point): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (rgbd): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (rgbn): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (fmri): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (imu): Sequential(
        (0): Linear(in_features=1024, out_features=1024, bias=True)
        (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (clip_proj2): ModuleDict(
      (image): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (video): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (audio): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (point): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (rgbd): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (rgbn): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (fmri): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
      (imu): Sequential(
        (0): Linear(in_features=1024, out_features=4096, bias=True)
        (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
    )
    (routers): ModuleDict(
      (image): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (video): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (audio): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (point): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (rgbd): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (rgbn): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (fmri): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
      (imu): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=4096, out_features=3, bias=True)
      )
    )
    (start_tag): ParameterDict(
        (image): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (video): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (audio): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (point): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (rgbd): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (rgbn): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (fmri): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (imu): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
    )
    (end_tag): ParameterDict(
        (image): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (video): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (audio): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (point): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (rgbd): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (rgbn): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (fmri): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
        (imu): Parameter containing: [torch.cuda.HalfTensor of size 1x1x4096 (GPU 0)]
    )
  )
)